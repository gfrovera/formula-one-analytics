{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastF1 Functionality Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import fastf1\n",
    "import pandas as pd\n",
    "import pyarrow.csv as csv\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import duckdb as db\n",
    "import yaml\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "# Configuring Logging\n",
    "logging.basicConfig(\n",
    "    encoding='utf-8',\n",
    "    level=logging.ERROR,\n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p'\n",
    ")\n",
    "\n",
    "CACHE_PATH = Path('/workspaces/formula-one-analytics/data/.cache/')\n",
    "TMP_PATH = Path('/workspaces/formula-one-analytics/data/_tmp/')\n",
    "\n",
    "# fastF1 data cache config\n",
    "if not CACHE_PATH.exists():\n",
    "    logging.info('CACHE_PATH does not exist creating...')\n",
    "    CACHE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not TMP_PATH.exists():\n",
    "    logging.info('TMP_PATH does not exist creating...')\n",
    "    TMP_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fastf1.Cache.enable_cache('/workspaces/formula-one-analytics/data/.cache/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formatting and Saving Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pandas_dtype': 'int64', 'duckdb_dtype': 'BIGINIT'},\n",
       " {'pandas_dtype': 'int32', 'duckdb_dtype': 'INTEGER'},\n",
       " {'pandas_dtype': 'float64', 'duckdb_dtype': 'DOUBLE'},\n",
       " {'pandas_dtype': 'float32', 'duckdb_dtype': 'FLOAT'},\n",
       " {'pandas_dtype': 'bool', 'duckdb_dtype': 'BOOLEAN'},\n",
       " {'pandas_dtype': 'datetime64[ns]', 'duckdb_dtype': 'TIMESTAMP'},\n",
       " {'pandas_dtype': 'timedelta64[ns]', 'duckdb_dtype': 'INTERVAL'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML File Importers\n",
    "def yaml_loaders(file_path: str, header: str) -> list:\n",
    "    with open(file_path, 'r') as yaml_file:\n",
    "        data = yaml.safe_load(yaml_file)\n",
    "\n",
    "        return data[header]\n",
    "    \n",
    "\n",
    "# Defining function to event calendar and save to parquet for later\n",
    "def get_event_calendar(start_year: int, end_year: int, iteration_sleep: int=10) -> list:\n",
    "    '''\n",
    "    Sources a list of dataframes with Formula 1 Event Calendar schedule information fro\n",
    "    the fastF1 API package and API.\n",
    "\n",
    "    :param start_year: (int) Beginning year of range to pull event calendar schedules for\n",
    "    :param end_year: (int) End year of range to pull event calendar schedules for\n",
    "    :iteration_sleep: (int, default=10) Adjustable sleep interval to keep fastF1 api from\n",
    "    blocking requests for data. fastF1 has protection built in, this allows for further \n",
    "    request safety.\n",
    "    '''\n",
    "    totalEventList = []\n",
    "    for idx, year in enumerate(list(range(start_year, end_year+1))):\n",
    "        totalEventList.append(fastf1.get_event_schedule(year))\n",
    "        logging.info(f'Requesting the {year=} event schedule')\n",
    "        sleep(iteration_sleep) # to throttle requests beyond built in fastF1 throttling\n",
    "\n",
    "    return totalEventList\n",
    "\n",
    "\n",
    "def concat_event_calendar(event_list: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    Concats data from list of dataframes into a single dataframe\n",
    "\n",
    "    :param event_list: (list) list of dataframes to concat into single dataframe\n",
    "    :return: pandas Dataframe\n",
    "\n",
    "    '''\n",
    "\n",
    "    df = pd.concat(event_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_date_part_col(df: pd.DataFrame, date_column: str, date_part:str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Creates a new column in supplied pd.DataFrame that is the specified DatetimeIndex part \n",
    "    of the given date_column.\n",
    "\n",
    "    :param df: (pd.DataFrame) pandas DataFrame containing a date or datetime column\n",
    "    :param date_column: (str) The name of the column with dates to extract the date_part from\n",
    "    :param date_part: (str) The part of the date to extract ('year', 'month','day')\n",
    "\n",
    "    :return: pandas DataFrame with the extracted date part in a new column.\n",
    "\n",
    "    ::Example::\n",
    "    -----------\n",
    "\n",
    "    data = [{'eventDate': '2000-03-12', 'event': 'Woodstock'}, \n",
    "            {'eventDate': '2003-08-12', 'event': 'EdgeFest'}, \n",
    "            {'eventDate': '2009-03-12', 'event': 'Warped Tour'}, \n",
    "            {'eventDate': '2020-01-26', 'event': 'Electric Forest'}\n",
    "       ]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['eventDate'] = pd.to_datetime(df['eventDate'])\n",
    "\n",
    "    |  eventDate  |    event           |\n",
    "    | 2000-03-12  | 'Woodstock'        |\n",
    "    | 2003-08-12  | 'EdgeFest'         |\n",
    "    | 2009-03-12  | 'Warped Tour'      |\n",
    "    | 2020-01-26  | 'Electric Forest'  |\n",
    "\n",
    "    df = create_date_part_col(df=df, date_column='eventDate', date_part='year')\n",
    "\n",
    "    |  eventDate  |    event           | eventDate_year |\n",
    "    | 2000-03-12  | 'Woodstock'        |      2000      |\n",
    "    | 2003-08-12  | 'EdgeFest'         |      2003      |\n",
    "    | 2009-03-12  | 'Warped Tour'      |      2009      |\n",
    "    | 2020-01-26  | 'Electric Forest'  |      2020      |\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    valid_date_parts = {'day', 'month', 'year'}\n",
    "    if date_part not in valid_date_parts:\n",
    "        raise ValueError(f'Given date_part must be a valid date part of {valid_date_parts}')\n",
    "\n",
    "    df['_'.join([date_column, date_part.lower()])] = (\n",
    "        getattr(pd.DatetimeIndex(df[date_column]), date_part)\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def temp_csver(df: pd.DataFrame, path: str|Path, temp_file_name: str, **kwargs) -> None:\n",
    "    '''\n",
    "    Takes pd.DataFrame, path and filename string and saves a csv file to that location.\n",
    "    Used instead of pd.to_csv() to handle using pathlib.Path objects with filenames.\n",
    "    By default pd.to_csv() does not allow for the path_or_buf argument to be overloaded.\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param path: (str|Path) path to location where csv will be saved\n",
    "    :param temp_file_name: (str) the filename of temp .csv file to be written\n",
    "    :param header: (bool) csv to have header True|False\n",
    "\n",
    "    :return: NoneType\n",
    "    '''\n",
    "\n",
    "    if isinstance(path, Path):\n",
    "        path_and_file = path/temp_file_name\n",
    "    elif isinstance(path, str):\n",
    "        path_and_file = '/'.join([path, temp_file_name])\n",
    "        \n",
    "    return df.to_csv(path_and_file, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Event Schedules by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling down Formula Event Calendars from 2000 to 2023 seasons.\n",
    "my_event_list = get_event_calendar(2000, 2024)\n",
    "eventDF = concat_event_calendar(my_event_list)\n",
    "\n",
    "# Adding new date part columns for parquet partitioning optimization\n",
    "eventDF = create_date_part_col(eventDF, 'EventDate', 'year')\n",
    "eventDF = create_date_part_col(eventDF, 'EventDate', 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data to parquet file for easier access later.\n",
    "eventDF.to_parquet('/workspaces/formula-one-analytics/data/eventCalendar.parquet',\n",
    "                   partition_cols=['EventDate_year', 'EventDate_month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Race Data Temp File Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "core           INFO \tLoading data for Bahrain Grand Prix - Practice 1 [v3.1.6]\n",
      "INFO:fastf1.fastf1.core:Loading data for Bahrain Grand Prix - Practice 1 [v3.1.6]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "INFO:fastf1.fastf1.req:Using cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "INFO:fastf1.fastf1.req:Using cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "INFO:fastf1.fastf1.req:Using cached data for session_status_data\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "INFO:fastf1.fastf1.req:Using cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "INFO:fastf1.fastf1.req:Using cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "INFO:fastf1.fastf1.req:Using cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "INFO:fastf1.fastf1.core:Processing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "INFO:fastf1.fastf1.req:Using cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "INFO:fastf1.fastf1.req:Using cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "INFO:fastf1.fastf1.req:Using cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "INFO:fastf1.fastf1.req:Using cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '10', '11', '14', '16', '18', '2', '20', '21', '22', '23', '24', '27', '31', '4', '44', '55', '63', '77', '81']\n",
      "INFO:fastf1.fastf1.core:Finished loading data for 20 drivers: ['1', '10', '11', '14', '16', '18', '2', '20', '21', '22', '23', '24', '27', '31', '4', '44', '55', '63', '77', '81']\n"
     ]
    }
   ],
   "source": [
    "my_race_session = fastf1.get_session(year=2023, gp=1, identifier=1)\n",
    "# my_race_session.load(telemetry=False, laps=False, weather=False, messages=False)\n",
    "my_race_session.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting session lap data, save to temp file\n",
    "sessionLabData = pd.DataFrame(my_race_session.laps)\n",
    "# temp_csver(df=y, path=TMP_PATH, temp_file_name='temp_laps.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mapping dict to convert pandas to duckdb\n",
    "dtypeConversions = yaml_loaders(file_path= '../src/schemas/DtypeMappings.yaml', header='mappings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time                  timedelta64[ns]\n",
       "Driver                         object\n",
       "DriverNumber                   object\n",
       "LapTime               timedelta64[ns]\n",
       "LapNumber                     float64\n",
       "Stint                         float64\n",
       "PitOutTime            timedelta64[ns]\n",
       "PitInTime             timedelta64[ns]\n",
       "Sector1Time           timedelta64[ns]\n",
       "Sector2Time           timedelta64[ns]\n",
       "Sector3Time           timedelta64[ns]\n",
       "Sector1SessionTime    timedelta64[ns]\n",
       "Sector2SessionTime    timedelta64[ns]\n",
       "Sector3SessionTime    timedelta64[ns]\n",
       "SpeedI1                       float64\n",
       "SpeedI2                       float64\n",
       "SpeedFL                       float64\n",
       "SpeedST                       float64\n",
       "IsPersonalBest                   bool\n",
       "Compound                       object\n",
       "TyreLife                      float64\n",
       "FreshTyre                        bool\n",
       "Team                           object\n",
       "LapStartTime          timedelta64[ns]\n",
       "LapStartDate           datetime64[ns]\n",
       "TrackStatus                    object\n",
       "Position                      float64\n",
       "Deleted                          bool\n",
       "DeletedReason                  object\n",
       "FastF1Generated                  bool\n",
       "IsAccurate                       bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessionLabData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaCreator:\n",
    "\n",
    "    @dataclass\n",
    "    class DtypeMapping:\n",
    "        pandas_dtype: str\n",
    "        duckdb_dtype: str\n",
    "\n",
    "    def __init__(self, dtype_mappings: list, schema_title: str) -> None:\n",
    "        self.dtype_mappings = [self.DtypeMapping(**m) for m in dtype_mappings]\n",
    "        self.schema_title = schema_title\n",
    "\n",
    "    def convert_to_duckdb_dtypes(self, pandas_dtypes):\n",
    "        for mapping in self.dtype_mappings:\n",
    "            if pandas_dtypes.name == mapping.pandas_dtype:\n",
    "                return mapping.duckdb_dtype\n",
    "        return 'VARCHAR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dtype_mappings = yaml_loaders(file_path= '../src/schemas/DtypeMappings.yaml', header='mappings')\n",
    "converter = SchemaCreator(schema_dtype_mappings, 'sessionLapData')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [1.1, 2.2, 3.3],\n",
    "    'C': ['a', 'b', 'c']\n",
    "})\n",
    "\n",
    "\n",
    "schema_parts = []\n",
    "for col, series in xdf.items():\n",
    "    col_dtype = series.dtype\n",
    "\n",
    "    duckdb_dtype = converter.convert_to_duckdb_dtypes(col_dtype)\n",
    "    schema_parts.append(f'{col} {duckdb_dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A      int64\n",
       "B    float64\n",
       "C     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A BIGINIT', 'B DOUBLE', 'C VARCHAR']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting car meta-data, combining list[dataframe] into single dataframe and saving to temp .csv file/\n",
    "c = my_race_session.car_data\n",
    "car_data_df = pd.concat([df.assign(Key=key) for key, df in c.items()],\n",
    "                        ignore_index=True)\n",
    "temp_csver(df=car_data_df, path=TMP_PATH, temp_file_name='temp_car_data.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackStsTmp = pd.DataFrame(my_race_session.track_status)\n",
    "temp_csver(df=trackStsTmp, path=TMP_PATH, temp_file_name='temp_track_status.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceResults = my_race_session.results\n",
    "temp_csver(df=raceResults, path=TMP_PATH, temp_file_name='temp_race_results.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceControlMsg = my_race_session.race_control_messages\n",
    "temp_csver(df=raceControlMsg, path=TMP_PATH, temp_file_name='temp_race_control_msg.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherData = my_race_session.weather_data\n",
    "temp_csver(df=weatherData, path=TMP_PATH, temp_file_name='temp_weather_data.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "positionData = my_race_session.pos_data\n",
    "pos_data_df = pd.concat([df.assign(Key=key) for key, df in positionData.items()],\n",
    "                        ignore_index=True)\n",
    "temp_csver(df=pos_data_df, path=TMP_PATH, temp_file_name='temp_pos_data.csv', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching Circuit Info\n",
    "circuitInfo = my_race_session.get_circuit_info()\n",
    "\n",
    "# Getting session event series, convert to DF and save to temp PDF\n",
    "evntDF = pd.DataFrame(my_race_session.event).T\n",
    "evntDF.columns = my_race_session.event.index\n",
    "evntDF['t0_date'] = my_race_session.t0_date\n",
    "evntDF['map_rotation'] = circuitInfo.rotation\n",
    "\n",
    "temp_csver(df=evntDF, path=TMP_PATH, temp_file_name='temp_events.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circuit info coordinate markers\n",
    "temp_csver(df=circuitInfo.corners, path=TMP_PATH, temp_file_name='temp_track_corners.csv', header=True, index=False)\n",
    "temp_csver(df=circuitInfo.marshal_sectors, path=TMP_PATH, temp_file_name='temp_marshal_sectors.csv', header=True, index=False)\n",
    "temp_csver(df=circuitInfo.marshal_lights, path=TMP_PATH, temp_file_name='temp_marshal_lights.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp Data Convert to Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "carDataTable = csv.read_csv('/workspaces/formula-one-analytics/data/_tmp/temp_car_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'RPM',\n",
       " 'Speed',\n",
       " 'nGear',\n",
       " 'Throttle',\n",
       " 'Brake',\n",
       " 'DRS',\n",
       " 'Source',\n",
       " 'Time',\n",
       " 'SessionTime',\n",
       " 'Key']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carDataTable.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\n",
    "    'session_date',\n",
    "    'engine_rpm',\n",
    "    'car_speed',\n",
    "    'car_ngear',\n",
    "    'car_throttle',\n",
    "    'car_braking',\n",
    "    'drs',\n",
    "    'source',\n",
    "    'time',\n",
    "    'session_time',\n",
    "    'key'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pa.schema([\n",
    "    ('session_date', pa.timestamp('ns')),\n",
    "    ('engine_rpm', pa.int64()),\n",
    "    ('car_speed', pa.int64()),\n",
    "    ('car_ngear', pa.int64()),\n",
    "    ('car_throttle', pa.int64()),\n",
    "    ('car_braking', pa.int8()),\n",
    "    ('drs', pa.int64()),\n",
    "    ('source', pa.string()),\n",
    "    ('time', pa.string()),\n",
    "    ('session_time', pa.string()),\n",
    "    ('key', pa.int64())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file '/fomula_one_project/data/2023/race_01/car_data.parquet'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m carDataTable \u001b[38;5;241m=\u001b[39m carDataTable\u001b[38;5;241m.\u001b[39mrename_columns(new_column_names)\n\u001b[1;32m      2\u001b[0m carDataTable \u001b[38;5;241m=\u001b[39m carDataTable\u001b[38;5;241m.\u001b[39mcast(schema)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarDataTable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/fomula_one_project/data/2023/race_01/car_data.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/lib/python3.11/site-packages/pyarrow/parquet/core.py:1883\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, **kwargs)\u001b[0m\n\u001b[1;32m   1881\u001b[0m use_int96 \u001b[38;5;241m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1883\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_page_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_int96\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1905\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1906\u001b[0m \u001b[43m            \u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorting_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1907\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m   1908\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(table, row_group_size\u001b[38;5;241m=\u001b[39mrow_group_size)\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/venv/lib/python3.11/site-packages/pyarrow/parquet/core.py:1004\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, **options)\u001b[0m\n\u001b[1;32m    999\u001b[0m         sink \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_handle \u001b[38;5;241m=\u001b[39m filesystem\u001b[38;5;241m.\u001b[39mopen(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# ARROW-10480: do not auto-detect compression.  While\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# a filename like foo.parquet.gz is nonconforming, it\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;66;03m# shouldn't implicitly apply compression.\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m         sink \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_handle \u001b[38;5;241m=\u001b[39m \u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_output_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1007\u001b[0m     sink \u001b[38;5;241m=\u001b[39m where\n",
      "File \u001b[0;32m/venv/lib/python3.11/site-packages/pyarrow/_fs.pyx:878\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.open_output_stream\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/venv/lib/python3.11/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/venv/lib/python3.11/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file '/fomula_one_project/data/2023/race_01/car_data.parquet'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "carDataTable = carDataTable.rename_columns(new_column_names)\n",
    "carDataTable = carDataTable.cast(schema)\n",
    "pq.write_table(carDataTable, '/fomula_one_project/data/2023/race_01/car_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
